{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pandas import json_normalize\n",
    "import yaml\n",
    "from yaml import CSafeLoader as Loader\n",
    "\n",
    "sns.set_context(\"paper\", font_scale=3)\n",
    "\n",
    "LOG_DIR = os.path.expanduser(\"~/jean-zay/emergent_communication/lightning_logs/\")\n",
    "# LOG_DIR = os.path.expanduser(\"~/PhD/emergent_communication/emergent_communication/lightning_logs_cluster/\")\n",
    "\n",
    "REFERENCE_METRIC = \"val_acc\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T15:16:22.808606028Z",
     "start_time": "2023-09-28T15:16:22.611867315Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/258 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considering pre-final results for run 42218\n",
      "considering pre-final results for run 41781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/258 [00:00<00:42,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considering pre-final results for run 40703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/258 [00:00<00:41,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considering pre-final results for run 48643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 7/258 [00:01<00:57,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considering pre-final results for run 46211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 8/258 [00:01<01:12,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considering pre-final results for run 46512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 13/258 [00:03<01:05,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considering pre-final results for run 48593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 14/258 [00:03<01:04,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considering pre-final results for run 44320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 19/258 [00:04<01:02,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considering pre-final results for run 31649\n",
      "considering pre-final results for run 42201\n",
      "considering pre-final results for run 45752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 21/258 [00:04<00:45,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considering pre-final results for run 46213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 24/258 [00:05<00:36,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considering pre-final results for run 41788\n",
      "considering pre-final results for run 21572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 28/258 [00:06<01:01,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considering pre-final results for run 40731\n",
      "considering pre-final results for run 41825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 30/258 [00:06<00:53,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considering pre-final results for run 40705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 31/258 [00:06<00:51,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considering pre-final results for run 37109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 32/258 [00:07<01:40,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considering pre-final results for run 31549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 33/258 [00:08<02:08,  1.75it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Remove superfluous NaN cells\n",
    "def compress(values):\n",
    "    for val in values:\n",
    "        if val is not None and not np.isnan(val):\n",
    "            return val\n",
    "        \n",
    "results_df = []\n",
    "for run_dir in tqdm(os.listdir(LOG_DIR)):\n",
    "    run_path = os.path.join(LOG_DIR, run_dir, \"checkpoints/\")\n",
    "\n",
    "    results = glob.glob(run_path+f\"*{REFERENCE_METRIC}=*.pickle\")\n",
    "    if len(results) < 1:\n",
    "        run_path = os.path.join(LOG_DIR, run_dir)\n",
    "        results = glob.glob(run_path+f\"/*.pickle\")\n",
    "        if len(results) >= 1:\n",
    "            print(f\"considering pre-final results for run {run_dir[8:]}\")\n",
    "            if run_dir[8:] == \"31521\":\n",
    "                print(\"gre\")\n",
    "\n",
    "    for result in results:\n",
    "        try:\n",
    "            data = pd.read_pickle(result)\n",
    "        except Exception as e:\n",
    "            print(result)\n",
    "            print(e)\n",
    "            continue\n",
    "            \n",
    "        if isinstance(data, list):\n",
    "            df = pd.DataFrame.from_records(data)\n",
    "            df = df.groupby([\"dir_name\", \"epoch\"]).aggregate(compress)\n",
    "        else:\n",
    "            if \"receiver_aux_loss\" in data.keys():\n",
    "                data[\"receiver_auxiliary_loss\"] = data[\"receiver_aux_loss\"]\n",
    "                del data[\"receiver_aux_loss\"]\n",
    "            df = pd.DataFrame.from_records([{k: v.item() if torch.is_tensor(v) else v for k, v in data.items()}])\n",
    "        \n",
    "        df[\"dir_name\"] = run_dir\n",
    "        if \"epoch\" in result:\n",
    "            df[\"epoch\"] = int(result.split(\"epoch=\")[1].split(\"-\")[0])\n",
    "        results_df.append(df)\n",
    "\n",
    "\n",
    "results_df = pd.concat(results_df, ignore_index=True)\n",
    "\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-09-28T15:16:22.653225672Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"version_31521\" in results_df.dir_name.values"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp = []\n",
    "for run_dir in tqdm(os.listdir(LOG_DIR)):\n",
    "    file_path = os.path.join(LOG_DIR, run_dir, \"hparams.yaml\")\n",
    "    file = yaml.load(open(file_path), Loader=Loader) #safe_load(, Loader=Loader)\n",
    "    df = json_normalize(file)\n",
    "    df[\"dir_name\"] = run_dir\n",
    "    hp.append(df)\n",
    "\n",
    "hp = pd.concat(hp, ignore_index=True)\n",
    "hp"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fix_duplicate_value(val, allow_offset=None):\n",
    "    if isinstance(val, list):\n",
    "        for el in val:\n",
    "            if allow_offset is None:\n",
    "                assert (el == val[0]) or (el == \"None\") or (val[0] == \"None\")\n",
    "            else:\n",
    "                assert (np.abs(el - val[0]) < allow_offset) or (el == \"None\") or (val[0] == \"None\")\n",
    "        return val[0]\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "hp = hp.applymap(fix_duplicate_value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "indices_best_steps = results_df.groupby(\"dir_name\")[REFERENCE_METRIC].idxmax()\n",
    "\n",
    "df = results_df.loc[list(indices_best_steps)].copy()\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.set_index(\"dir_name\", inplace=True, drop=False)\n",
    "if not hp.index.name == \"dir_name\":\n",
    "    hp.set_index(\"dir_name\", inplace=True, verify_integrity=True)\n",
    "df = df.join(hp, how=\"left\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert (df.sender_entropy_coeff == df.receiver_entropy_coeff).all()\n",
    "assert (df.num_senders == df.num_receivers).all()\n",
    "assert (df.sender_layer_norm == df.receiver_layer_norm).all()\n",
    "\n",
    "df[\"entropy_coeff\"] = df[\"sender_entropy_coeff\"]\n",
    "df[\"num_agents\"] = df[\"num_senders\"]\n",
    "df[\"layer_norm\"] = df[\"sender_layer_norm\"]\n",
    "\n",
    "df[\"attr_val\"] = df[\"num_attributes\"].map(int).map(str) + \"_\" + df[\"num_values\"].map(int).map(str)\n",
    "\n",
    "data = df"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_capacity(row):\n",
    "    return math.pow(row.num_values, row.num_attributes)\n",
    "\n",
    "data[\"capacity\"] = data.apply(calc_capacity, axis=1)\n",
    "data.sort_values(\"capacity\", inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[\"test_acc\"] = data[\"val_acc\"]\n",
    "data[\"test_acc_no_noise\"] = data[\"val_acc_no_noise\"]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[\"condition\"] = data[\"noise\"].map(lambda x: f\"noise_{x}\" if x > 0 else \"baseline\") + data[\"feedback\"].map(lambda x: \"_feedback\" if x else \"\") + data[\"receiver_aux_loss\"].map(lambda x: \"_aux_loss\" if x else \"\")\n",
    "\n",
    "target_data = data.copy()\n",
    "\n",
    "target_data = target_data[target_data.receiver_aux_loss == False]\n",
    "\n",
    "NUM_AGENTS = 1\n",
    "target_data = target_data[target_data.num_agents == NUM_AGENTS]\n",
    "\n",
    "MAX_LEN = 10\n",
    "target_data = target_data[target_data.max_len == MAX_LEN]\n",
    "\n",
    "NOISE = 0.5\n",
    "target_data = target_data[target_data.noise == NOISE]\n",
    "\n",
    "VOCAB_SIZE = 2\n",
    "target_data = target_data[target_data.vocab_size == VOCAB_SIZE]\n",
    "\n",
    "VOCAB_SIZE_FEEDBACK = 2\n",
    "target_data = target_data[(target_data.vocab_size_feedback == VOCAB_SIZE_FEEDBACK) | (target_data.feedback == False)]\n",
    "\n",
    "LAYER_NORM = 1\n",
    "target_data = target_data[target_data.layer_norm == LAYER_NORM]\n",
    "\n",
    "DISCRIMINATION_NUM_OBJECTS = 2\n",
    "target_data = target_data[target_data.discrimination_num_objects == DISCRIMINATION_NUM_OBJECTS]\n",
    "\n",
    "# hue_order = None\n",
    "# hue_order = [\"baseline\", \"noise_0.1\"]\n",
    "hue_order = [\"noise_0.5\", \"noise_0.5_feedback\", \"noise_0.5_feedback_aux_loss\"]\n",
    "\n",
    "# order = [\"2_10\", \"3_5\", \"2_16\", \"4_4\", \"4_5\", \"3_10\", \"2_100\"]\n",
    "# order = [\"2_100\", \"4_100\", \"10_100\"]\n",
    "order = [\"2_10\", \"4_4\", \"3_10\", \"2_100\", \"2_1000\", \"10_1000\"]\n",
    "\n",
    "target_hparam = \"attr_val\"\n",
    "\n",
    "num_runs_data = target_data.groupby([\"attr_val\", \"condition\"]).size().reset_index()\n",
    "\n",
    "\n",
    "metrics = [\"test_acc\", \"test_acc_no_noise\", \"topsim\"]\n",
    "# metrics = [\"test_acc\", \"topsim\", \"topsim_sender_receiver\", \"posdis\", \"bosdis\", \"test_acc_no_noise\", \"train_acc_no_noise\"]\n",
    "_, axes = plt.subplots(1, len(metrics), figsize=(35, 10))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axis = axes[i]\n",
    "    # g = sns.barplot(ax=axis, data=target_data, x=target_hparam, y=metric, order=order, hue=\"feedback\", dodge=True)\n",
    "    g = sns.pointplot(ax=axis, data=target_data, x=target_hparam, y=metric, order=order, hue=\"feedback\", errorbar=\"se\", linestyles=\"\", dodge=True)\n",
    "\n",
    "    if not i == 0:\n",
    "        g.legend().remove()\n",
    "    else:\n",
    "        sns.move_legend(axis, \"lower left\")\n",
    "    axis.set_title(metric)\n",
    "    axis.set_ylabel(\"\")\n",
    "    # handles, labels = ax.get_legend_handles_labels()\n",
    "    # num_conditions = int(len(handles)/2) if not hue_order else len(hue_order)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/effect_of_attr_val.pdf\", dpi=300)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_data = data.copy()\n",
    "\n",
    "target_data = target_data[target_data.receiver_aux_loss == False]\n",
    "\n",
    "NUM_AGENTS = 1\n",
    "target_data = target_data[target_data.num_agents == NUM_AGENTS]\n",
    "\n",
    "MAX_LEN = 10\n",
    "target_data = target_data[target_data.max_len == MAX_LEN]\n",
    "\n",
    "VOCAB_SIZE = 2\n",
    "target_data = target_data[target_data.vocab_size == VOCAB_SIZE]\n",
    "\n",
    "VOCAB_SIZE_FEEDBACK = 2\n",
    "target_data = target_data[(target_data.vocab_size_feedback == VOCAB_SIZE_FEEDBACK) | (target_data.feedback == False)]\n",
    "\n",
    "LAYER_NORM = 1\n",
    "target_data = target_data[target_data.layer_norm == LAYER_NORM]\n",
    "\n",
    "DISCRIMINATION_NUM_OBJECTS = 2\n",
    "target_data = target_data[target_data.discrimination_num_objects == DISCRIMINATION_NUM_OBJECTS]\n",
    "\n",
    "ATTR_VAL = \"4_4\"\n",
    "# ATTR_VAL = \"3_10\"\n",
    "\n",
    "target_data = target_data[(target_data[\"attr_val\"] == ATTR_VAL)]\n",
    "\n",
    "# print(target_data.dir_name.unique())\n",
    "target_hparam = \"noise\"\n",
    "order = [0, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "\n",
    "num_runs_data = target_data.groupby([\"attr_val\", \"condition\"]).size().reset_index()\n",
    "\n",
    "metrics = [\"test_acc\", \"test_acc_no_noise\", \"topsim\", \"posdis\", \"bosdis\"]\n",
    "# metrics = [\"test_acc\", \"topsim\", \"topsim_sender_receiver\", \"posdis\", \"bosdis\", \"test_acc_no_noise\", \"train_acc_no_noise\"]\n",
    "_, axes = plt.subplots(1, len(metrics), figsize=(35, 10))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axis = axes[i]\n",
    "    # g = sns.barplot(ax=axis, data=target_data, x=target_hparam, y=metric, order=order, hue=\"feedback\", dodge=True)\n",
    "    g = sns.pointplot(ax=axis, data=target_data, x=target_hparam, y=metric, order=order, hue=\"feedback\", errorbar=\"se\")\n",
    "\n",
    "    if not i == 0:\n",
    "        g.legend().remove()\n",
    "    else:\n",
    "        sns.move_legend(axis, \"lower left\")\n",
    "    axis.set_title(metric)\n",
    "    axis.set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/effect_of_noise.pdf\", dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_data = data.copy()\n",
    "\n",
    "target_data = target_data[target_data.receiver_aux_loss == False]\n",
    "\n",
    "NUM_AGENTS = 1\n",
    "target_data = target_data[target_data.num_agents == NUM_AGENTS]\n",
    "\n",
    "NOISE = 0.7\n",
    "target_data = target_data[target_data.noise == NOISE]\n",
    "\n",
    "VOCAB_SIZE = 2\n",
    "target_data = target_data[target_data.vocab_size == VOCAB_SIZE]\n",
    "\n",
    "VOCAB_SIZE_FEEDBACK = 2\n",
    "target_data = target_data[(target_data.vocab_size_feedback == VOCAB_SIZE_FEEDBACK) | (target_data.feedback == False)]\n",
    "\n",
    "LAYER_NORM = 1\n",
    "target_data = target_data[target_data.layer_norm == LAYER_NORM]\n",
    "\n",
    "DISCRIMINATION_NUM_OBJECTS = 2\n",
    "target_data = target_data[target_data.discrimination_num_objects == DISCRIMINATION_NUM_OBJECTS]\n",
    "\n",
    "ATTR_VAL = \"4_4\"\n",
    "\n",
    "target_data = target_data[(target_data[\"attr_val\"] == ATTR_VAL)]\n",
    "\n",
    "# target_hparam = \"attr_val\"\n",
    "# order = [\"2_10\", \"4_4\", \"2_100\", \"2_1000\"]\n",
    "target_hparam = \"max_len\"\n",
    "order = [1, 3, 5, 10, 20, 30]\n",
    "\n",
    "\n",
    "num_runs_data = target_data.groupby([\"attr_val\", \"condition\"]).size().reset_index()\n",
    "\n",
    "metrics = [\"test_acc\", \"test_acc_no_noise\", \"topsim\"]\n",
    "# metrics = [\"test_acc\", \"topsim\", \"topsim_sender_receiver\", \"posdis\", \"bosdis\", \"test_acc_no_noise\", \"train_acc_no_noise\"]\n",
    "_, axes = plt.subplots(1, len(metrics), figsize=(35, 10))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axis = axes[i]\n",
    "    # g = sns.barplot(ax=axis, data=target_data, x=target_hparam, y=metric, order=order, hue=\"feedback\", dodge=True)\n",
    "    g = sns.pointplot(ax=axis, data=target_data, x=target_hparam, y=metric, order=order, hue=\"feedback\", errorbar=\"se\")\n",
    "\n",
    "    if not i == 0:\n",
    "        g.legend().remove()\n",
    "    else:\n",
    "        sns.move_legend(axis, \"lower left\")\n",
    "    axis.set_title(metric)\n",
    "    axis.set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/effect_of_message_length.pdf\", dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_data = data.copy()\n",
    "\n",
    "target_data = target_data[target_data.guesswhat == True]\n",
    "\n",
    "target_data = target_data[target_data.receiver_output_attention == False]\n",
    "target_data = target_data[target_data.receiver_object_attention == False]\n",
    "target_data = target_data[target_data.sender_attention == False]\n",
    "\n",
    "target_data = target_data[(target_data.receiver_aux_loss == False)]\n",
    "\n",
    "NUM_AGENTS = 1\n",
    "target_data = target_data[target_data.num_agents == NUM_AGENTS]\n",
    "\n",
    "MAX_LEN = 10\n",
    "target_data = target_data[target_data.max_len == MAX_LEN]\n",
    "\n",
    "# NOISE = 0.5\n",
    "# target_data = target_data[target_data.noise == NOISE]\n",
    "\n",
    "VOCAB_SIZE = 2\n",
    "target_data = target_data[target_data.vocab_size == VOCAB_SIZE]\n",
    "\n",
    "VOCAB_SIZE_FEEDBACK = 2\n",
    "target_data = target_data[(target_data.vocab_size_feedback == VOCAB_SIZE_FEEDBACK) | (target_data.feedback == False)]\n",
    "\n",
    "LAYER_NORM = 1\n",
    "target_data = target_data[target_data.layer_norm == LAYER_NORM]\n",
    "\n",
    "DISCRIMINATION_NUM_OBJECTS = 10\n",
    "target_data = target_data[target_data.discrimination_num_objects == DISCRIMINATION_NUM_OBJECTS]\n",
    "\n",
    "# target_hparam = \"attr_val\"\n",
    "# order = [\"2_10\", \"4_4\", \"2_100\", \"2_1000\"]\n",
    "target_hparam = \"noise\"\n",
    "order = [0, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "metrics = [\"test_acc\", \"test_acc_no_noise\", \"train_acc\"]\n",
    "# metrics = [\"test_acc\", \"topsim\", \"topsim_sender_receiver\", \"posdis\", \"bosdis\", \"test_acc_no_noise\", \"train_acc_no_noise\"]\n",
    "_, axes = plt.subplots(1, len(metrics), figsize=(35, 10))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axis = axes[i]\n",
    "    # g = sns.barplot(ax=axis, data=target_data, x=target_hparam, y=metric, order=order, hue=\"feedback\", dodge=True)\n",
    "    g = sns.pointplot(ax=axis, data=target_data, x=target_hparam, y=metric, order=order, hue=\"feedback\", errorbar=\"se\")\n",
    "\n",
    "    if not i == 0:\n",
    "        g.legend().remove()\n",
    "    else:\n",
    "        sns.move_legend(axis, \"lower left\")\n",
    "    axis.set_title(metric)\n",
    "    axis.set_ylabel(\"\")\n",
    "    axis.set_ylim((0, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/guesswhat_noise.pdf\", dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_data = data.copy()\n",
    "\n",
    "target_data = target_data[target_data.guesswhat == True]\n",
    "\n",
    "target_data = target_data[target_data.receiver_output_attention == False]\n",
    "target_data = target_data[target_data.receiver_object_attention == False]\n",
    "target_data = target_data[target_data.sender_attention == False]\n",
    "\n",
    "target_data = target_data[target_data.receiver_aux_loss == False]\n",
    "\n",
    "\n",
    "NUM_AGENTS = 1\n",
    "target_data = target_data[target_data.num_agents == NUM_AGENTS]\n",
    "\n",
    "NOISE = 0.5\n",
    "target_data = target_data[target_data.noise == NOISE]\n",
    "\n",
    "VOCAB_SIZE = 2\n",
    "target_data = target_data[target_data.vocab_size == VOCAB_SIZE]\n",
    "\n",
    "VOCAB_SIZE_FEEDBACK = 2\n",
    "target_data = target_data[(target_data.vocab_size_feedback == VOCAB_SIZE_FEEDBACK) | (target_data.feedback == False)]\n",
    "\n",
    "LAYER_NORM = 1\n",
    "target_data = target_data[target_data.layer_norm == LAYER_NORM]\n",
    "\n",
    "DISCRIMINATION_NUM_OBJECTS = 10\n",
    "target_data = target_data[target_data.discrimination_num_objects == DISCRIMINATION_NUM_OBJECTS]\n",
    "\n",
    "# target_hparam = \"attr_val\"\n",
    "# order = [\"2_10\", \"4_4\", \"2_100\", \"2_1000\"]\n",
    "target_hparam = \"max_len\"\n",
    "order = [1, 5, 10, 20, 30, 50]\n",
    "\n",
    "metrics = [\"test_acc\", \"test_acc_no_noise\", \"train_acc\"]\n",
    "# metrics = [\"test_acc\", \"topsim\", \"topsim_sender_receiver\", \"posdis\", \"bosdis\", \"test_acc_no_noise\", \"train_acc_no_noise\"]\n",
    "_, axes = plt.subplots(1, len(metrics), figsize=(35, 10))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axis = axes[i]\n",
    "    # g = sns.barplot(ax=axis, data=target_data, x=target_hparam, y=metric, order=order, hue=\"feedback\", dodge=True)\n",
    "    g = sns.pointplot(ax=axis, data=target_data, x=target_hparam, y=metric, order=order, hue=\"feedback\", errorbar=\"se\")\n",
    "\n",
    "    if not i == 0:\n",
    "        g.legend().remove()\n",
    "    else:\n",
    "        sns.move_legend(axis, \"lower left\")\n",
    "    axis.set_title(metric)\n",
    "    axis.set_ylabel(\"\")\n",
    "    axis.set_ylim((0, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/guesswhat_max_len.pdf\", dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_data = data.copy()\n",
    "\n",
    "target_data = target_data[target_data.guesswhat == True]\n",
    "\n",
    "target_data = target_data[target_data.receiver_output_attention == False]\n",
    "target_data = target_data[target_data.receiver_object_attention == False]\n",
    "target_data = target_data[target_data.sender_attention == False]\n",
    "\n",
    "target_data = target_data[(target_data.receiver_aux_loss == False)]\n",
    "\n",
    "NUM_AGENTS = 1\n",
    "target_data = target_data[target_data.num_agents == NUM_AGENTS]\n",
    "\n",
    "MAX_LEN = 10\n",
    "target_data = target_data[target_data.max_len == MAX_LEN]\n",
    "\n",
    "VOCAB_SIZE = 10\n",
    "target_data = target_data[target_data.vocab_size == VOCAB_SIZE]\n",
    "\n",
    "VOCAB_SIZE_FEEDBACK = 2\n",
    "target_data = target_data[(target_data.vocab_size_feedback == VOCAB_SIZE_FEEDBACK) | (target_data.feedback == False)]\n",
    "\n",
    "LAYER_NORM = 1\n",
    "target_data = target_data[target_data.layer_norm == LAYER_NORM]\n",
    "\n",
    "DISCRIMINATION_NUM_OBJECTS = 10\n",
    "target_data = target_data[target_data.discrimination_num_objects == DISCRIMINATION_NUM_OBJECTS]\n",
    "\n",
    "# target_hparam = \"attr_val\"\n",
    "# order = [\"2_10\", \"4_4\", \"2_100\", \"2_1000\"]\n",
    "target_hparam = \"noise\"\n",
    "order = [0, 0.5]\n",
    "\n",
    "metrics = [\"test_acc\", \"test_acc_no_noise\", \"train_acc\"]\n",
    "# metrics = [\"test_acc\", \"topsim\", \"topsim_sender_receiver\", \"posdis\", \"bosdis\", \"test_acc_no_noise\", \"train_acc_no_noise\"]\n",
    "_, axes = plt.subplots(1, len(metrics), figsize=(35, 10))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axis = axes[i]\n",
    "    # g = sns.barplot(ax=axis, data=target_data, x=target_hparam, y=metric, order=order, hue=\"feedback\", dodge=True)\n",
    "    g = sns.pointplot(ax=axis, data=target_data, x=target_hparam, y=metric, order=order, hue=\"feedback\", errorbar=\"se\")\n",
    "\n",
    "    if not i == 0:\n",
    "        g.legend().remove()\n",
    "    else:\n",
    "        sns.move_legend(axis, \"lower left\")\n",
    "    axis.set_title(metric)\n",
    "    axis.set_ylabel(\"\")\n",
    "    axis.set_ylim((0, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/guesswhat_noise_vocab_high.pdf\", dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
