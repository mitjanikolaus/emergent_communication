{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pandas import json_normalize\n",
    "import yaml\n",
    "from yaml import CSafeLoader as Loader\n",
    "\n",
    "sns.set_context(\"paper\", font_scale=3)\n",
    "\n",
    "LOG_DIR = os.path.expanduser(\"~/lis-cluster/emergent_communication/emergent_communication/lightning_logs/\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 346/780 [00:43<01:55,  3.75it/s]"
     ]
    }
   ],
   "source": [
    "results_df = []\n",
    "for run_dir in tqdm(os.listdir(LOG_DIR)):\n",
    "    run_path = os.path.join(LOG_DIR, run_dir, \"checkpoints/\")\n",
    "\n",
    "    results = glob.glob(run_path+\"*.pickle\")\n",
    "    for result in results:\n",
    "        df = pd.read_pickle(result)\n",
    "        df = pd.DataFrame.from_records(df)\n",
    "        df[\"dir_name\"] = run_dir\n",
    "        df[\"epoch\"] = int(result.split(\"epoch=\")[1].split(\"-\")[0])\n",
    "        results_df.append(df)\n",
    "\n",
    "\n",
    "results_df = pd.concat(results_df, ignore_index=True)\n",
    "\n",
    "# Remove superfluous NaN cells\n",
    "def compress(values):\n",
    "    for val in values:\n",
    "        if val is not None and not np.isnan(val):\n",
    "            return val\n",
    "\n",
    "results_df = results_df.groupby([\"dir_name\", \"epoch\"]).aggregate(compress)\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 780/780 [00:20<00:00, 38.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "    accelerator accumulate_grad_batches amp_backend amp_level  auto_lr_find  \\\n0          None                    None      native      None         False   \n1          None                    None      native      None         False   \n2          None                    None      native      None         False   \n3          None                    None      native      None         False   \n4          None                    None      native      None         False   \n..          ...                     ...         ...       ...           ...   \n775        None                    None      native      None         False   \n776        None                    None      native      None         False   \n777        None                    None      native      None         False   \n778        None                    None      native      None         False   \n779        None                    None      native      None         False   \n\n     auto_scale_batch_size  auto_select_gpus baseline_type  batch_size  \\\n0                    False             False          mean        5120   \n1                    False             False          mean        5120   \n2                    False             False          mean        5120   \n3                    False             False          mean        5120   \n4                    False             False          mean        5120   \n..                     ...               ...           ...         ...   \n775                  False             False          mean        5120   \n776                  False             False          mean        5120   \n777                  False             False          mean        5120   \n778                  False             False          mean        5120   \n779                  False             False          mean        5120   \n\n    benchmark  ...  tpu_cores track_grad_norm val_check_interval  vocab_size  \\\n0        None  ...       None              -1               None           5   \n1        None  ...       None              -1               None           5   \n2        None  ...       None              -1               None           5   \n3        None  ...       None              -1               None           5   \n4        None  ...       None              -1               None           5   \n..        ...  ...        ...             ...                ...         ...   \n775      None  ...       None              -1               None         100   \n776      None  ...       None              -1               None         100   \n777      None  ...       None              -1               None         100   \n778      None  ...       None              -1               None         100   \n779      None  ...       None              -1               None         100   \n\n     vocab_size_feedback weights_save_path  weights_summary         dir_name  \\\n0                      3              None              top  version_1060567   \n1                      2              None              top  version_1060572   \n2                      2              None              top  version_1060573   \n3                      3              None              top  version_1060570   \n4                      2              None              top  version_1060568   \n..                   ...               ...              ...              ...   \n775                    2              None              top  version_1073800   \n776                    3              None              top  version_1073799   \n777                    3              None              top  version_1073795   \n778                    2              None              top  version_1073796   \n779                    2              None              top  version_1073797   \n\n     receiver_layer_norm  sender_layer_norm  \n0                    NaN                NaN  \n1                    NaN                NaN  \n2                    NaN                NaN  \n3                    NaN                NaN  \n4                    NaN                NaN  \n..                   ...                ...  \n775                 True               True  \n776                 True               True  \n777                 True               True  \n778                 True               True  \n779                 True               True  \n\n[780 rows x 98 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>accelerator</th>\n      <th>accumulate_grad_batches</th>\n      <th>amp_backend</th>\n      <th>amp_level</th>\n      <th>auto_lr_find</th>\n      <th>auto_scale_batch_size</th>\n      <th>auto_select_gpus</th>\n      <th>baseline_type</th>\n      <th>batch_size</th>\n      <th>benchmark</th>\n      <th>...</th>\n      <th>tpu_cores</th>\n      <th>track_grad_norm</th>\n      <th>val_check_interval</th>\n      <th>vocab_size</th>\n      <th>vocab_size_feedback</th>\n      <th>weights_save_path</th>\n      <th>weights_summary</th>\n      <th>dir_name</th>\n      <th>receiver_layer_norm</th>\n      <th>sender_layer_norm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>None</td>\n      <td>None</td>\n      <td>native</td>\n      <td>None</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>mean</td>\n      <td>5120</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>-1</td>\n      <td>None</td>\n      <td>5</td>\n      <td>3</td>\n      <td>None</td>\n      <td>top</td>\n      <td>version_1060567</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>None</td>\n      <td>None</td>\n      <td>native</td>\n      <td>None</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>mean</td>\n      <td>5120</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>-1</td>\n      <td>None</td>\n      <td>5</td>\n      <td>2</td>\n      <td>None</td>\n      <td>top</td>\n      <td>version_1060572</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>None</td>\n      <td>None</td>\n      <td>native</td>\n      <td>None</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>mean</td>\n      <td>5120</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>-1</td>\n      <td>None</td>\n      <td>5</td>\n      <td>2</td>\n      <td>None</td>\n      <td>top</td>\n      <td>version_1060573</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>None</td>\n      <td>None</td>\n      <td>native</td>\n      <td>None</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>mean</td>\n      <td>5120</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>-1</td>\n      <td>None</td>\n      <td>5</td>\n      <td>3</td>\n      <td>None</td>\n      <td>top</td>\n      <td>version_1060570</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>None</td>\n      <td>None</td>\n      <td>native</td>\n      <td>None</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>mean</td>\n      <td>5120</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>-1</td>\n      <td>None</td>\n      <td>5</td>\n      <td>2</td>\n      <td>None</td>\n      <td>top</td>\n      <td>version_1060568</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>775</th>\n      <td>None</td>\n      <td>None</td>\n      <td>native</td>\n      <td>None</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>mean</td>\n      <td>5120</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>-1</td>\n      <td>None</td>\n      <td>100</td>\n      <td>2</td>\n      <td>None</td>\n      <td>top</td>\n      <td>version_1073800</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>776</th>\n      <td>None</td>\n      <td>None</td>\n      <td>native</td>\n      <td>None</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>mean</td>\n      <td>5120</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>-1</td>\n      <td>None</td>\n      <td>100</td>\n      <td>3</td>\n      <td>None</td>\n      <td>top</td>\n      <td>version_1073799</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>777</th>\n      <td>None</td>\n      <td>None</td>\n      <td>native</td>\n      <td>None</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>mean</td>\n      <td>5120</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>-1</td>\n      <td>None</td>\n      <td>100</td>\n      <td>3</td>\n      <td>None</td>\n      <td>top</td>\n      <td>version_1073795</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>778</th>\n      <td>None</td>\n      <td>None</td>\n      <td>native</td>\n      <td>None</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>mean</td>\n      <td>5120</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>-1</td>\n      <td>None</td>\n      <td>100</td>\n      <td>2</td>\n      <td>None</td>\n      <td>top</td>\n      <td>version_1073796</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>779</th>\n      <td>None</td>\n      <td>None</td>\n      <td>native</td>\n      <td>None</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>mean</td>\n      <td>5120</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>-1</td>\n      <td>None</td>\n      <td>100</td>\n      <td>2</td>\n      <td>None</td>\n      <td>top</td>\n      <td>version_1073797</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>780 rows × 98 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp = []\n",
    "for run_dir in tqdm(os.listdir(LOG_DIR)):\n",
    "    file_path = os.path.join(LOG_DIR, run_dir, \"hparams.yaml\")\n",
    "    file = yaml.load(open(file_path), Loader=Loader) #safe_load(, Loader=Loader)\n",
    "    df = json_normalize(file)\n",
    "    df[\"dir_name\"] = run_dir\n",
    "    hp.append(df)\n",
    "\n",
    "hp = pd.concat(hp, ignore_index=True)\n",
    "hp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fix_duplicate_value(val, allow_offset=None):\n",
    "    if isinstance(val, list):\n",
    "        for el in val:\n",
    "            if allow_offset is None:\n",
    "                assert (el == val[0]) or (el == \"None\") or (val[0] == \"None\")\n",
    "            else:\n",
    "                assert (np.abs(el - val[0]) < allow_offset) or (el == \"None\") or (val[0] == \"None\")\n",
    "        return val[0]\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "hp = hp.applymap(fix_duplicate_value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# REFERENCE_METRIC = \"val_acc_no_noise\"\n",
    "REFERENCE_METRIC = \"val_acc\"\n",
    "\n",
    "\n",
    "indices_best_steps = results_df.groupby(\"dir_name\")[REFERENCE_METRIC].idxmax()\n",
    "\n",
    "df = results_df.loc[list(indices_best_steps)].copy()\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.set_index(\"dir_name\", inplace=True, drop=False)\n",
    "if not hp.index.name == \"dir_name\":\n",
    "    hp.set_index(\"dir_name\", inplace=True, verify_integrity=True)\n",
    "df = df.join(hp, how=\"left\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAX_N_RUNS = 10\n",
    "df.dropna(subset=[\"num_attributes\", \"num_values\"], inplace=True)\n",
    "df.fillna({\"sender_layer_norm\": 0, \"receiver_layer_norm\": 0}, inplace=True)\n",
    "\n",
    "assert (df.sender_entropy_coeff == df.receiver_entropy_coeff).all()\n",
    "assert (df.num_senders == df.num_receivers).all()\n",
    "assert (df.sender_layer_norm == df.receiver_layer_norm).all()\n",
    "\n",
    "df[\"entropy_coeff\"] = df[\"sender_entropy_coeff\"]\n",
    "df[\"num_agents\"] = df[\"num_senders\"]\n",
    "df[\"layer_norm\"] = df[\"sender_layer_norm\"]\n",
    "\n",
    "runs_best_entropy = []\n",
    "\n",
    "df[\"attr_val\"] = df[\"num_attributes\"].map(int).map(str) + \"_\" + df[\"num_values\"].map(int).map(str)\n",
    "attr_val_combinations = df[\"attr_val\"].unique()\n",
    "\n",
    "num_agents_values = df[\"num_agents\"].unique()\n",
    "for num_agents in num_agents_values:\n",
    "    print(f\"\\nNum agents: {num_agents}\")\n",
    "\n",
    "    for attr_val in attr_val_combinations:\n",
    "        n_attributes = int(float(attr_val.split(\"_\")[0]))\n",
    "        n_values = int(float(attr_val.split(\"_\")[1]))\n",
    "        print(f\"\\n\\t\\tAttr: {n_attributes} Values: {n_values}\")\n",
    "\n",
    "        # length_cost_values = df[\"length_cost\"].unique()\n",
    "        length_cost_values = [0, 0.001]\n",
    "        for length_cost in length_cost_values:\n",
    "            for layer_norm in [0, 1]:\n",
    "                max_len_values = df.max_len.unique()\n",
    "                for max_len in max_len_values:\n",
    "                    vocab_size_values = df.vocab_size.unique()\n",
    "                    for vocab_size in vocab_size_values:\n",
    "                        noise_values = df.noise.unique()\n",
    "                        for noise in noise_values:\n",
    "                            for feedback in (0, 1):\n",
    "                                for self_repair in (0, 1):\n",
    "\n",
    "                                    df_config = df[(df.attr_val == attr_val) & (df.length_cost == length_cost) & (df.feedback == feedback) & (df.num_agents == num_agents) & (df.noise == noise) & (df.self_repair == self_repair) & (df.max_len == max_len) & (df.vocab_size == vocab_size) & (df.layer_norm == layer_norm)]\n",
    "\n",
    "                                    if len(df_config) == 0:\n",
    "                                        continue\n",
    "\n",
    "                                    print(f\"\\t\\t\\tLength cost: {length_cost}\\t Noise: {noise}\\tmax_len: {max_len}\\t vocab_size: {vocab_size}\\t layer_norm: {layer_norm}\", end=\"\")\n",
    "\n",
    "                                    if feedback:\n",
    "                                        print(\"\\tFeedback\", end=\"\")\n",
    "                                    elif self_repair:\n",
    "                                        print(\"\\tSelf repair\", end=\"\")\n",
    "                                    else:\n",
    "                                        print(\"\\tBaseline\", end=\"\")\n",
    "\n",
    "                                    print(f\"\\tFound {len(df_config)} runs\") #: {df_config}\n",
    "\n",
    "                                    avg_val_accs = df_config.groupby(\"sender_entropy_coeff\").aggregate({REFERENCE_METRIC: \"mean\"})\n",
    "                                    # Take the highest entropy coeff in case of tie\n",
    "                                    best_entropy_coeff = avg_val_accs[avg_val_accs[REFERENCE_METRIC] == avg_val_accs.max()[0]].index[-1]\n",
    "\n",
    "                                    df_best_entropy = df_config[df_config.entropy_coeff == best_entropy_coeff]\n",
    "                                    print(f\"\\t\\t\\t\\t\\t\\tbest entropy coeff: {best_entropy_coeff}; num runs: {len(df_best_entropy)}\", end=\"\")\n",
    "                                    print(f\"\\tother: {avg_val_accs.to_dict()})\", )\n",
    "\n",
    "                                    if len(df_best_entropy) > MAX_N_RUNS:\n",
    "                                        df_best_entropy = df_best_entropy.tail(10)\n",
    "                                    runs_best_entropy.append(df_best_entropy)\n",
    "\n",
    "data = pd.concat(runs_best_entropy, ignore_index=True)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_capacity(row):\n",
    "    return math.pow(row.num_values, row.num_attributes)\n",
    "\n",
    "data[\"capacity\"] = data.apply(calc_capacity, axis=1)\n",
    "data.sort_values(\"capacity\", inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[\"condition\"] = data[\"noise\"].map(lambda x: f\"noise_{x}\" if x > 0 else \"baseline\") + data[\"length_cost\"].map(lambda x: f\"_length_cost_{x}\" if x > 0 else \"\") + data[\"feedback\"].map(lambda x: \"_feedback\" if x else \"\") + data[\"self_repair\"].map(lambda x: \"_self_repair\" if x else \"\")\n",
    "\n",
    "target_data = data.copy()\n",
    "\n",
    "NUM_AGENTS = 1\n",
    "target_data = target_data[target_data.num_agents == NUM_AGENTS]\n",
    "\n",
    "MAX_LEN = 5\n",
    "target_data = target_data[target_data.max_len == MAX_LEN]\n",
    "\n",
    "VOCAB_SIZE = 100\n",
    "target_data = target_data[target_data.vocab_size == VOCAB_SIZE]\n",
    "\n",
    "LAYER_NORM = 1\n",
    "target_data = target_data[target_data.layer_norm == LAYER_NORM]\n",
    "\n",
    "\n",
    "# ATTR_VAL = \"4_5\"\n",
    "# target_data = target_data[(target_data[\"attr_val\"] == ATTR_VAL)]\n",
    "\n",
    "# print(target_data.dir_name.unique())\n",
    "\n",
    "# hue_order = None\n",
    "# hue_order = [\"baseline\", \"noise_0.1\", \"noise_0.1_feedback_binary\"]\n",
    "hue_order = [\"baseline\", \"noise_0.1\", \"noise_0.1_self_repair\", \"noise_0.1_feedback\"]\n",
    "\n",
    "# order = [\"2_10\", \"3_5\", \"2_16\", \"4_4\", \"4_5\", \"3_10\", \"2_100\"]\n",
    "order = [\"2_16\"]\n",
    "\n",
    "\n",
    "target_hparam = \"attr_val\"\n",
    "\n",
    "num_runs_data = target_data.groupby([\"attr_val\", \"condition\"]).size().reset_index()\n",
    "# plt.figure(figsize=(30, 10))\n",
    "\n",
    "_, axes = plt.subplots(10, 1, figsize=(50, 150))\n",
    "\n",
    "sns.boxplot(ax=axes[0], data=num_runs_data, x=\"attr_val\", order=order, hue=\"condition\", hue_order=hue_order, y=0)\n",
    "\n",
    "# next_axis_idx = (0, 0)\n",
    "for i, metric in enumerate([\"val_acc\", \"test_acc_no_noise\", \"topsim\", \"posdis\", \"bosdis\", \"test_acc\", \"val_acc_no_noise\", \"train_acc_no_noise\", \"sender_entropy_coeff\"]):\n",
    "    sns.boxplot(ax=axes[i+1], data=target_data, x=target_hparam, y=metric, order=order, hue=\"condition\", hue_order=hue_order, boxprops=dict(alpha=.5), showfliers = False)\n",
    "    ax = sns.swarmplot(ax=axes[i+1], data=target_data, x=target_hparam, y=metric, order=order, hue=\"condition\", hue_order=hue_order, dodge=True)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    num_conditions = int(len(handles)/2) if not hue_order else len(hue_order)\n",
    "    ax.legend(handles[:num_conditions], labels[:num_conditions])\n",
    "    # plt.setp(ax.get_legend().get_texts(), fontsize='22')\n",
    "    # if next_axis_idx[1] >= axes.shape[1]-1:\n",
    "    #     next_axis_idx = (next_axis_idx[0]+1, 0)\n",
    "    # else:\n",
    "    #     next_axis_idx = (next_axis_idx[0], next_axis_idx[1]+1)\n",
    "\n",
    "name = \"results\"\n",
    "plt.savefig(\"plots/\"+name+\".pdf\", dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
